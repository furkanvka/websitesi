# Optimizasyon Teknikleri

## TÃ¼rev TabanlÄ± Tekniklere Derinlemesine BakÄ±ÅŸ

### Gradient Descent

Gradient descent parametlerin maliyet fonksiyonun tÃ¼revi ile gÃ¼ncellenmesi ile minimum bulunmasÄ±nÄ± amaÃ§layan optimizsasyon methodur.matematiksel tanÄ±mÄ± Burada:

* $\theta$: parametre vektÃ¶rÃ¼
* $\eta$: Ã¶ÄŸrenme oranÄ± (learning rate)
* $\nabla_\theta f(\theta)$: gradyan vektÃ¶rÃ¼


$$
\theta \leftarrow \theta - \eta \nabla_\theta f(\theta)
$$

AdÄ±m adÄ±m bur sÃ¼reci anlatmamÄ±z gerekeirse:

1-) Her parametre iÃ§in KayÄ±p Fonksiyonun (loss function) tÃ¼revini al.

2-) Parametreler iÃ§in rastgele deÄŸerler topla ve toplanan deÄŸerleri parametrelere gÃ¶nder.

3-)AdÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (step size) hesapla.

4-) Yeni parametreyi hesapla.

5-) Minimum seviyeye ulaÅŸasÄ±ya kadar 2. adÄ±ma dÃ¶n.

![asd](gradiant.webp)


**Steepest descent**, genellikle, Ã¶ÄŸrenme oranÄ± $\eta$'nÄ±n her adÄ±mda negatif gradyan yÃ¶nÃ¼nde maksimum kazanÃ§ saÄŸlayacak ÅŸekilde seÃ§ildiÄŸi gradient descent tÃ¼rÃ¼ olarak tanÄ±mlanÄ±r.
Bu $\eta$'nÄ±n (adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼n) her iterasyonda nasÄ±l belirleneceÄŸini araÅŸtÄ±ran kÄ±sma **line search** denir.

---

### Steepest Descent

* **Gradient descent** yalnÄ±zca negatif gradyan yÃ¶nÃ¼nde bir azalma saÄŸlar.
* **Steepest gradient descent** ise, fonksiyonun en bÃ¼yÃ¼k yÃ¶nlÃ¼ tÃ¼revine gÃ¶re azalma yapar.

**En dik iniÅŸ yÃ¶ntemi**, bir fonksiyonun minimumunu bulmak iÃ§in kullanÄ±lan iteratif bir optimizasyon algoritmasÄ±dÄ±r. Her adÄ±mda fonksiyonun gradyanÄ±nÄ±n tersi yÃ¶nÃ¼nde ilerleyerek minimuma doÄŸru hareket edilir.

#### AdÄ±m BÃ¼yÃ¼klÃ¼ÄŸÃ¼ ($\epsilon$)

Yeni noktaya hangi bÃ¼yÃ¼klÃ¼kte adÄ±m atÄ±lacaÄŸÄ±nÄ± belirler:

$$
x_{\text{yeni}} = x_{\text{eski}} - \epsilon \cdot \nabla f
$$

Burada $\epsilon$, genellikle $\frac{d}{d\epsilon} f(z(\epsilon)) = 0$ Ã§Ã¶zÃ¼lerek hesaplanÄ±r.

Diyelim ki bir tepenin eteklerindeyiz ve ne kadar ilerlememiz gerektigini hesaplalmÄ±z gerekiyorTepedeki eÄŸim sana hangi yÃ¶ne gitmen gerektiÄŸini sÃ¶yler (gradient yÃ¶nÃ¼).Ama ne kadar uzaÄŸa gitmen gerektiÄŸini seÃ§mek iÃ§in, adÄ±mÄ±nÄ± bÃ¼yÃ¼tÃ¼p kÃ¼Ã§Ã¼lttÃ¼ÄŸÃ¼nde yolda ne kadar aÅŸaÄŸÄ± indiÄŸine bakarsÄ±n.En Ã§ok aÅŸaÄŸÄ± indiÄŸin adÄ±m uzunluÄŸunda durursun.Matematikte bu, fonksiyonun adÄ±m uzunluÄŸuna gÃ¶re tÃ¼revini alÄ±p sÄ±fÄ±ra eÅŸitlemekle aynÄ± ÅŸeydir.

#### Ã–rnek 1: $f(x) = x^4$

* $z(\epsilon) = x - \epsilon \cdot 4x^3$
* EÄŸer minimum noktanÄ±n $0$ olduÄŸu biliniyorsa:

$$
\epsilon = \frac{1}{4x^2}
$$

Yine baÅŸlangÄ±Ã§tan baÄŸÄ±msÄ±z olarak tek adÄ±mda minimuma ulaÅŸÄ±lÄ±r.

---

#### Ã–rnek 1: 2 boyutlu â€” $f(x_1,x_2)=x_1^2+3x_2^2$

- Gradyan:

$$
\nabla f = \begin{bmatrix} 2x_1 \\ 6x_2 \end{bmatrix}
$$

- Arama yÃ¶nÃ¼:

$$
X_{\text{yeni}} = X - t \nabla f
= \begin{bmatrix}
x_1 - 2 t x_1 \\
x_2 - 6 t x_2
\end{bmatrix}
= \begin{bmatrix}
x_1 (1 - 2t) \\
x_2 (1 - 6t)
\end{bmatrix}
$$

$$
z(t) = \begin{bmatrix} x_1(1-2t) \\ x_2(1-6t) \end{bmatrix}
$$

- Line search sonucunda optimal $t$:



$$
t = \frac{x_1^2 + 9x_2^2}{2x_1^2 + 54x_2^2}
$$

* Ä°terasyon:

$$
X_{n+1} = X_n - t \cdot \nabla f
$$

---

### Aradaki farkÄ± gÃ¶rmek python

```python 
import numpy as np
import matplotlib.pyplot as plt

# Fonksiyon ve gradient
def f(x):
    return (x[0] - 3)**2 + (x[1] + 1)**2

def grad_f(x):
    return np.array([2*(x[0] - 3), 2*(x[1] + 1)])

# Gradient descent sabit adÄ±mla
def gradient_descent_step(x, eta):
    return x - eta * grad_f(x)

# Steepest descent line search ile
def steepest_descent_step(x):
    d = -grad_f(x)
    # line search: burada minimize edilecek fonksiyon
    phi = lambda alpha: f(x + alpha * d)
    # Ã§ok kaba bir search
    alphas = np.linspace(0, 1, 100)
    phi_values = [phi(a) for a in alphas]
    best_alpha = alphas[np.argmin(phi_values)]
    return x + best_alpha * d

# BaÅŸlangÄ±Ã§ noktasÄ±
x0 = np.array([0.0, 0.0])

# Ä°terasyonlar
steps = 20
eta = 0.1

gd_points = [x0]
sd_points = [x0]

x_gd = x0.copy()
x_sd = x0.copy()

for i in range(steps):
    x_gd = gradient_descent_step(x_gd, eta)
    x_sd = steepest_descent_step(x_sd)
    gd_points.append(x_gd)
    sd_points.append(x_sd)

gd_points = np.array(gd_points)
sd_points = np.array(sd_points)

# Plot
plt.figure(figsize=(8,6))
X, Y = np.meshgrid(np.linspace(-1,5,100), np.linspace(-3,2,100))
Z = (X - 3)**2 + (Y + 1)**2
plt.contour(X, Y, Z, levels=30)

plt.plot(gd_points[:,0], gd_points[:,1], 'o-', label='Gradient Descent (sabit adÄ±m)')
plt.plot(sd_points[:,0], sd_points[:,1], 's-', label='Steepest Descent (line search)')

plt.plot(3, -1, 'rx', markersize=12, label='Minimum')

plt.legend()
plt.title('Gradient Descent vs Steepest Descent')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.show()
```

---

## Lineer YaklaÅŸÄ±m (Linear Approximation)

Bir fonksiyonu belirli bir noktada **doÄŸrusal** kabul ederek, deÄŸerini yaklaÅŸÄ±k hesaplamaya Ã§alÄ±ÅŸÄ±r. Bu, birinci dereceden Taylor aÃ§Ä±lÄ±mÄ±na dayanÄ±r:

$$
f(x+\Delta x) \approx f(x) + \Delta x \cdot f'(x)
$$

---

### Ã–rnek 1: $(1.0002)^{50}$

* $f(x)=x^{50}$, $x=1$, $\Delta x=0.0002$
* YaklaÅŸÄ±m:

$$
f(1.0002) \approx 1 + 0.0002 \cdot 50 = 1.01
$$
### Ã–rnek 2: $f(x)=\ln x$

* $x$ 1'e yakÄ±nsa:

$$
\ln x \approx x-1
$$

* $x$ 2'ye yakÄ±nsa:

$$
\ln x \approx \ln 2 + \frac{x-2}{2}
$$

### Daha iyi yaklaÅŸÄ±m iÃ§in Taylor serisi

Daha yÃ¼ksek dereceden serilerle daha doÄŸru sonuÃ§lar elde edilir:

$$
f(x+\Delta x) \approx \sum_{i=0}^{N} \frac{f^{(i)}(x)}{i!} (\Delta x)^i
$$

## Newton-Raphson Metodu

Bir fonksiyonun kÃ¶klerini ($f(x)=0$) veya ekstremumlarÄ±nÄ± ($f'(x)=0$) bulmak iÃ§in kullanÄ±lan iteratif bir yÃ¶ntemdir.


### 1. Dereceden (KÃ¶k Bulma)

$$
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
$$

* **Ã–rnek:** $\sqrt{2}$ bulmak iÃ§in $x^2-2=0$

$$
x_{n+1} = x_n - \frac{x_n^2 -2}{2x_n}
$$

$x_0=1$ ile baÅŸlayÄ±nca birkaÃ§ adÄ±mda $\approx 1.41421$.

### 2. Dereceden (Minimum/Maksimum Bulma)

$$
x_{n+1} = x_n - \frac{f'(x_n)}{f''(x_n)}
$$


### KarÅŸÄ±laÅŸtÄ±rma

| Ã–zellik        | Gradient Descent        | Newton-Raphson      |
| -------------- | ----------------------- | ------------------- |
| AdÄ±m BÃ¼yÃ¼klÃ¼ÄŸÃ¼ | Belirlenir ($\epsilon$) | Yok (otomatik)      |
| Hesaplama      | Daha az tÃ¼rev           | Daha fazla tÃ¼rev    |
| YakÄ±nsama      | Genelde yavaÅŸ           | Genelde hÄ±zlÄ±       |
| Risk           | Daha stabil             | DÃ¶ngÃ¼ (cycle) riski |

Ã–rneÄŸin, $f(x)=x^3-2x+2$ iÃ§in yanlÄ±ÅŸ baÅŸlangÄ±Ã§ noktalarÄ± yÃ¶ntemin dÃ¶ngÃ¼ye girmesine yol aÃ§abilir.

Tabii! Ä°ÅŸte o yazÄ±nÄ±n sonuna, adÄ±m adÄ±m Ã§alÄ±ÅŸma aÃ§Ä±klamasÄ±nÄ± eklenmiÅŸ hali:

---

## Levenberg-Marquardt (LM) algoritmasÄ±

Levenberg-Marquardt algoritmasÄ±, doÄŸrusal olmayan problemlerin karesal hata formÃ¼lleri ile optimizasyonu iÃ§in kullanÄ±lÄ±r.

Gradient descent ile Gauss-Newton yÃ¶nteminin bir kombinasyonu gibidir.

### âš™ AlgoritmanÄ±n Ã‡ekirdeÄŸi

Her iterasyonda $x$ iÃ§in ÅŸu lineer sistem Ã§Ã¶zÃ¼lÃ¼r:

$$
\bigl( J^T J + \lambda I \bigr) \delta = - J^T r
$$

* $J = \frac{\partial r}{\partial x}$: Jacobian matrisi ($m \times n$)
* $\lambda > 0$: damping parametresi
* $I$: birim matris

Yeni parametre:

$$
x_{\text{new}} = x + \delta
$$

### âš– Gauss-Newton & Gradient Descent KÃ¶prÃ¼sÃ¼

* KÃ¼Ã§Ã¼k $\lambda$ â†’ **Gauss-Newton gibi davranÄ±r** (hÄ±zlÄ±, ama kararsÄ±z olabilir).
* BÃ¼yÃ¼k $\lambda$ â†’ **gradient descent gibi davranÄ±r** (daha yavaÅŸ, ama daha gÃ¼venli).

### ğŸ”„ Adaptif $\lambda$

* EÄŸer adÄ±m **baÅŸarÄ±lÄ±** (yani hata azalÄ±yor) ise $\lambda$ **kÃ¼Ã§Ã¼ltÃ¼lÃ¼r** â†’ Gauss-Newtonâ€™a yaklaÅŸÄ±r.
* EÄŸer adÄ±m **baÅŸarÄ±sÄ±z** (hata artÄ±yor) ise $\lambda$ **bÃ¼yÃ¼tÃ¼lÃ¼r** â†’ gradient descentâ€™e yaklaÅŸÄ±r.

---

### ğŸš€ AlgoritmanÄ±n AdÄ±m AdÄ±m Ä°ÅŸleyiÅŸi

1. **BaÅŸlangÄ±Ã§**

   * BaÅŸlangÄ±Ã§ parametresi $x_0$ ve damping katsayÄ±sÄ± $\lambda_0$ seÃ§ilir.
   * Bir bÃ¼yÃ¼tme oranÄ± $\nu > 1$ (Ã¶r. 10) belirlenir.

2. **Her iterasyonda:**

   * **Residual vektÃ¶rÃ¼ ve Jacobian hesaplanÄ±r:**

     $$
     r(x) = \begin{bmatrix} r_1(x) \\ \vdots \\ r_m(x) \end{bmatrix}, 
     \quad 
     J = \frac{\partial r}{\partial x}
     $$
   * **Lineer sistem Ã§Ã¶zÃ¼lÃ¼r:**

     $$
     \bigl(J^T J + \lambda I\bigr) \delta = - J^T r
     $$
   * **Yeni parametre adayÄ± hesaplanÄ±r:**

     $$
     x_{\text{new}} = x + \delta
     $$

3. **Hata kontrol edilir:**

   * Yeni hata $E(x_{\text{new}}) = \frac12 \|r(x_{\text{new}})\|^2$ hesaplanÄ±r.
   * EÄŸer $E(x_{\text{new}}) < E(x)$:

     * $x$ gÃ¼ncellenir: $x \leftarrow x_{\text{new}}$
     * $\lambda$ azaltÄ±lÄ±r: $\lambda \leftarrow \lambda / \nu$
   * Aksi halde:

     * AdÄ±m reddedilir (eski $x$ korunur)
     * $\lambda$ artÄ±rÄ±lÄ±r: $\lambda \leftarrow \lambda \cdot \nu$

4. **YakÄ±nsama kontrol edilir:**

   * $\|\delta\|$ Ã§ok kÃ¼Ã§Ã¼kse veya hata yeterince azalmÄ±ÅŸsa algoritma durur.


```python
import numpy as np
from scipy.optimize import least_squares

# Model fonksiyonu
def model(x, t):
    return x[0] * np.exp(-x[1] * t) + x[2]

# Residual (hata) fonksiyonu
def residuals(x, t, y):
    return y - model(x, t)

# Ã–rnek veri
t_data = np.linspace(0, 5, 50)
y_data = 2 * np.exp(-1.3 * t_data) + 0.5 + 0.1*np.random.randn(len(t_data))

# BaÅŸlangÄ±Ã§ tahmini
x0 = [1, 1, 1]

# LM optimizasyon
result = least_squares(residuals, x0, args=(t_data, y_data), method='lm')

print("Bulunan parametreler:", result.x)
```

bu video Ã§ok iyi :
https://www.youtube.com/watch?v=UQsOyMj9lnI&t=19s

## BFGS

Ã‡ok geliÅŸmiÅŸ bir optimzasyon algoritmasÄ± Ã¶ptimizasyon kÃ¼tÃ¼phanelerindeki min() funk arkasÄ±nda Ã§alÄ±ÅŸan genel algoritmadÄ±r kendisi .Bir quasi-Newton yÃ¶ntemi olarak geÃ§er. quasi newron yÃ¶nteminin nasÄ±l Ã§alÄ±ÅŸtÄ±gÄ±ndan bahsedelim ilk:

- Newton yÃ¶ntemindeki gibi H^-1 âˆ‡f(x) (H = Hessian) kullanmak isteriz.

- Ama Hessian matrisini (âˆ‡Â²f) doÄŸrudan hesaplamak maliyetli ve bazen mÃ¼mkÃ¼n deÄŸil.

- Bunun yerine iterasyonlarda Hessian (ya da onun tersi) iÃ§in yaklaÅŸÄ±k bir matris B veya H tutulur ve her adÄ±mda gÃ¼ncellenir.

bfgs algoritmasÄ± bunu belli bir prensipte gerÃ§ekleÅŸtiri bunun nasÄ±l Ã§alÄ±ÅŸtÄ±hÄ±nÄ± anlamak iÃ§in adÄ±m adÄ±m inceleleyelim

### ğŸ” BFGS algoritmasÄ±nÄ±n adÄ±mlarÄ±

KÄ±saca:

1. BaÅŸlangÄ±Ã§ tahmini:

   * `x_0` (baÅŸlangÄ±Ã§ noktasÄ±)
   * `H_0 = I` (ters Hessian yaklaÅŸÄ±k matrisi genelde birim matris ile baÅŸlatÄ±lÄ±r).

2. Her iterasyonda:

   * Gradient hesaplanÄ±r: `g_k = âˆ‡f(x_k)`
   * AdÄ±m yÃ¶nÃ¼ hesaplanÄ±r:

     ```
     p_k = - H_k * g_k
     ```
   * Uygun adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (line search) ile `Î±_k` bulunur.
   * Yeni nokta:

     ```
     x_{k+1} = x_k + Î±_k * p_k
     ```
   * Gradient farkÄ±:

     ```
     y_k = âˆ‡f(x_{k+1}) - âˆ‡f(x_k)
     ```
   * Nokta farkÄ±:

     ```
     s_k = x_{k+1} - x_k
     ```
   * `H_k` gÃ¼ncellenir:

     ```
     Ï_k = 1 / (y_káµ— s_k)
     V_k = (I - Ï_k s_k y_káµ—)
     H_{k+1} = V_k H_k V_káµ— + Ï_k s_k s_káµ—
     ```

     (Bu formul **BFGS update formÃ¼lÃ¼** olarak bilinir.)

3. Kriter saÄŸlanmazsa (`||âˆ‡f||` kÃ¼Ã§Ã¼k deÄŸilse) bir sonraki iterasyona geÃ§ilir.


## KaynakÃ§a

- https://www.youtube.com/watch?v=QGFct_3HMzk
- https://www.mit.edu/~hlb/StantonGrant/Lecture9/quadratic.pdf
- https://www.youtube.com/watch?v=VIoWzHlz7k8
- https://www.youtube.com/watch?v=UQsOyMj9lnI&t=19s
- [Medium â€” Steepest Descent](https://medium.com/@habicoban/steepest-descent-algoritmas%C4%B1-fonksiyon-optimizasyonunda-bir-ad%C4%B1m-%C3%B6ne-ge%C3%A7mek-aac7bc58d4d0)
- https://medium.com/data-science/linear-regression-using-gradient-descent-97a6c8700931

